1、activation functions
    使用relu作为激活函数，可以避免梯度爆炸或梯度弥散问题，但是神经元可能部分死亡
  tanh 在任何方面都强于sigmoid(tanh关于0对称)，但两者都会在某些值较大或较小时出现梯度消失问题。
2、data processing
    对数据进行归一化处理（减去均值），使数据尽量零中心化
    每张图像减去均值得到均值图像，或是对每个通道减去均值得到通道均值图像（均为零中心化处理方法）
  对数据进行白化处理
3、weight initializing
    不能全0处理的原因：所有神经元完全对称，表现的完全一样，进行相同的运算，计算相同的梯度
    使用small random numbers(常用方法：使用标准差为0.01的高斯分布式)
    上述方法仅在神经网络层数较少时适用，随着神经网络层数的增加，网络对初始化方式更加敏感。较小初始值的初始化方式使得在反向传播的过程中，由于链
  式法则，梯度不断乘以w，当传播到较浅的层时梯度就趋向于消失-梯度弥散（均值和方差都下降到0）  
    Xavier initializing,对神经元的所有输入数量取根号求值，如果输入较多则初始值较小，输入较少则初始值较大
    w=np.random.randn(fan_in,fan_out)/sqrt(fan_in) (以tanh为激活函数)
    注意：该公式并不使用于relu，反而会使数据方差下降的更快，越来越多的神经元没有被激活
    relu使得数据的方差下降了一半（个人理解：relu使得大于0的数据维持原状，而小于0的数据则全部为0，在高斯分布的数据中有一半都变为了0，所有数据的
  的方差也自然下降到了原来的一半）
    解决办法：使用下列函数
    w=np.random.randn(fan_in,fan_out)/sqrt(fan_in/2)
4、batch normalization
    BN层接受输出，并使得输出呈现高斯分布
    计算了每个特征的均值和方差，规范化后用gamma与Beta对数据进行shift  y=gamma*x+beta
    防止了梯度爆炸和梯度弥散的问题出现
    当gamma=标准差，beta=均值时，BN层通过学习自动抵消了BN层本身的效果
    支持更高的学习率，起到了部分正则化的作用
5、babysitting your learning process
    取一个小型的数据集，放入网络中进行训练，观察网络是否能够对该小部分数据进行过拟合（如果不行，网络必然有问题）
    选择不同的学习率进行从重复实验，观察loss的降低情况。由此可以了解到学习率的选择是否正确
    通过交叉训练在一定区间内试验不同的超参数，从而得到最佳的超参数选择（正则化参数，学习率）
  